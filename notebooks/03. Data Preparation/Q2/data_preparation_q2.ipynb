{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c043a26",
   "metadata": {},
   "source": [
    "# 3. Data Preparation\n",
    "\n",
    "In this phase we will prepare the dataset, modifying it according to the previous phase. In the end of this notebook we should have a dataset ready to be used in modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceb5642",
   "metadata": {},
   "source": [
    "## 3.1 Select Data\n",
    "\n",
    "We begin by loading the complete raw data so that we have more flexibility in the preparation of the data.\n",
    "\n",
    "The selection of the most relevant variables will be done in the next steps."
   ]
  },
  {
   "cell_type": "code",
   "id": "fc2f42cf",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "student = pd.read_csv(\"../../../databases/2018/student2018.csv\")\n",
    "teacher = pd.read_csv(\"../../../databases/2018/only_teacher2018.csv\")\n",
    "\n",
    "#student = pd.read_csv(\"../../../databases/2018/student2018.csv\", nrows=2000)\n",
    "#teacher = pd.read_csv(\"../../../databases/2018/only_teacher2018.csv\", nrows=2000)\n",
    "\n",
    "print(\"student shape:\", student.shape)\n",
    "print(\"teacher shape:\", teacher.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1edaac1d",
   "metadata": {},
   "source": [
    "## 3.2 Clean Data\n",
    "\n",
    "From the previous phase, we decided to remove unnecessary variables according the following criteria:\n",
    "- Variables with more than 70% missing values.\n",
    "- Students and teachers from England, due to differences in their education system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bbfbe86cb5fc46",
   "metadata": {},
   "source": [
    "#### Students data"
   ]
  },
  {
   "cell_type": "code",
   "id": "f749c9de",
   "metadata": {},
   "source": [
    "missing = student.isnull().mean().sort_values(ascending=False)\n",
    "student = student.drop(columns=missing[missing > 0.7].index)\n",
    "\n",
    "student = student[(student[\"CNT\"] != \"b'GBR'\")]\n",
    "\n",
    "student.head(5)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**List of countries**:\n",
    "teacher dataset only has these countries (so we should remove all students from other countries):\n",
    "`ALB`, `QAZ`, `BRA`, `CHL`, `TAP`, `DOM`, `DEU`, `HKG`, `KOR`, `MAC`,`MYS`,`MAR`,`PAN`,`PER`,`PRT`,`ESP`,`ARE`,`GBR`,`USA`"
   ],
   "id": "98895f63c830ddca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "student = student[\n",
    "    student[\"CNT\"].isin(teacher['CNT'].unique())\n",
    "]\n",
    "\n",
    "print(student['CNT'].unique())"
   ],
   "id": "75c264f17d983876",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ac5805b5cdc57fc6",
   "metadata": {},
   "source": [
    "#### Teachers data"
   ]
  },
  {
   "cell_type": "code",
   "id": "50ba9d199f48c4ad",
   "metadata": {},
   "source": [
    "missing = teacher.isnull().mean().sort_values(ascending=False)\n",
    "teacher = teacher.drop(columns=missing[missing > 0.7].index)\n",
    "\n",
    "teacher = teacher[(teacher[\"CNT\"] != \"b'GBR'\")]\n",
    "\n",
    "teacher.head(5)\n",
    "\n",
    "del missing"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"student shape:\", student.shape)\n",
    "print(\"teacher shape:\", teacher.shape)"
   ],
   "id": "470f3e3189414709",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1835a248",
   "metadata": {},
   "source": [
    "## 3.3 Construct Data\n",
    "\n",
    "In this step, we construct new variables based on the available data.\n",
    "\n",
    "For each subject, there are 10 plausible values (PVs), which we averaged into a single score column per subject."
   ]
  },
  {
   "cell_type": "code",
   "id": "3a403736",
   "metadata": {},
   "source": [
    "from columns_list_q2 import (\n",
    "    get_avg_results,\n",
    "    drop_columns,\n",
    "    math_columns,\n",
    "    reading_columns,\n",
    "    science_columns\n",
    ")\n",
    "\n",
    "student = get_avg_results(student, reading_columns, \"Reading\", None)\n",
    "student = get_avg_results(student, math_columns, \"Math\", None)\n",
    "student = get_avg_results(student, science_columns, \"Science\", None)\n",
    "\n",
    "student = drop_columns(student)\n",
    "\n",
    "print(pd.to_numeric(student['Avg Reading Result'], errors='coerce').isna().sum())\n",
    "print(pd.to_numeric(student['Avg Science Result'], errors='coerce').isna().sum())\n",
    "print(pd.to_numeric(student['Avg Math Result'], errors='coerce').isna().sum())\n",
    "\n",
    "del math_columns\n",
    "del reading_columns\n",
    "del science_columns"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c21d8b04",
   "metadata": {},
   "source": "student.head(5)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"student shape:\", student.shape)\n",
    "print(\"teacher shape:\", teacher.shape)"
   ],
   "id": "bf56b9a549d03875",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4547863f",
   "metadata": {},
   "source": [
    "## 3.4 Integrate Data"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "First save data, to keep memory clean",
   "id": "6153d86673798175"
  },
  {
   "cell_type": "code",
   "id": "c584e9aeaef4399b",
   "metadata": {},
   "source": [
    "teacher.to_csv(\"../../../databases/2018/cleaned_teacher2018.csv\", index=False)\n",
    "student.to_csv(\"../../../databases/2018/cleaned_students2018.csv\", index=False)\n",
    "\n",
    "del teacher\n",
    "del student"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Join tables",
   "id": "33aa08e3f8aebd61"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "columns_to_keep = [\n",
    "    'Avg Math Result', 'Avg Science Result', 'Avg Reading Result\"'\n",
    "    'CNT', 'CNTSCHID', 'REPEAT', 'ST001D01T', 'CNTSTUID'\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "# Function to load and merge datasets in chunks\n",
    "def init_target_csv():\n",
    "    with open(\"../../../databases/2018/cleaned_teacher_student2018.csv\", 'w') as file:\n",
    "        pass # Do nothing, just open and close the file\n",
    "\n",
    "    df1 = pd.read_csv('../../../databases/2018/cleaned_students2018.csv', nrows=2)\n",
    "    df2 = pd.read_csv('../../../databases/2018/cleaned_teacher2018.csv', nrows=2)\n",
    "\n",
    "    # df1 = df1[columns_to_keep]\n",
    "\n",
    "    df_result = pd.DataFrame(columns=(df1.columns.append(df2.columns)).unique())\n",
    "    df_result.to_csv(\"../../../databases/2018/cleaned_teacher_student2018.csv\", index_label=False)\n",
    "\n",
    "# Function to load and merge datasets in chunks\n",
    "def get_dataset_in_chunks(chunk_size):\n",
    "    chunks = []\n",
    "    i=0\n",
    "    for chunk in pd.read_csv('../../../databases/2018/cleaned_students2018.csv', chunksize=chunk_size):\n",
    "        print(\"i=\",i,flush=True)\n",
    "        i=i+1\n",
    "\n",
    "        #chunk = chunk[columns_to_keep]\n",
    "\n",
    "        merged_chunk = pd.merge(chunk, pd.read_csv('../../../databases/2018/cleaned_teacher2018.csv'), on=['CNT', 'CNTSCHID'])\n",
    "        merged_chunk.to_csv(\"../../../databases/2018/cleaned_teacher_student2018.csv\", mode=\"a\", header=False, index=False)\n",
    "        #print(merged_chunk.head(), flush=True)\n",
    "        #chunks.append(merged_chunk)\n",
    "    #return pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "# Define the chunk size\n",
    "chunk_size = 50000 # Adjust the chunk size based on your memory capacity\n",
    "\n",
    "init_target_csv()\n",
    "# Perform the merge in chunks\n",
    "get_dataset_in_chunks(chunk_size)\n",
    "\n",
    "print(\"Merge completed and saved to 'cleaned_teacher_student2018.csv'.\")"
   ],
   "id": "f05a8ecc7754607d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "#load new table\n",
    "teacher_student = pd.read_csv('../../../databases/2018/cleaned_teacher_student2018.csv')\n"
   ],
   "id": "e8f8aaf63b1cf3b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Group by teacher (CNTTCHID) with students performance aggregated\n",
    "summary = teacher_student.groupby('CNTTCHID').agg(\n",
    "    total_students=('CNTSTUID', 'count'),\n",
    "    repeating_students=('REPEAT', 'sum'),\n",
    "    highest_reading_score=('Avg Reading Result', 'max'),\n",
    "    lowest_reading_score=('Avg Reading Result', 'min'),\n",
    "    average_reading_score=('Avg Reading Result', 'mean'),\n",
    "    average_science_score=('Avg Science Result', 'mean'),\n",
    "    average_math_score=('Avg Math Result', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "summary['percentage_repeating'] = (summary['repeating_students'] / summary['total_students']) * 100\n",
    "\n",
    "# Merge statistics back into the original teacher with student data aggregated\n",
    "teacher_student = teacher_student.merge(\n",
    "    summary[[\n",
    "        'CNTTCHID',\n",
    "        'percentage_repeating',\n",
    "        'total_students',\n",
    "        'repeating_students',\n",
    "        'highest_reading_score',\n",
    "        'lowest_reading_score',\n",
    "        'average_reading_score',\n",
    "        'average_science_score',\n",
    "        'average_math_score'\n",
    "    ]],\n",
    "    on='CNTTCHID', how='left')\n",
    "\n",
    "teacher_student.drop(columns=['Avg Reading Result', 'Avg Science Result', 'Avg Math Result', 'REPEAT'])\n",
    "teacher_student = teacher_student.groupby('CNTTCHID')\n",
    "\n",
    "#Save database with aggregated data\n",
    "teacher_student.to_csv(\"../../../databases/2018/integrated_teacher_student2018.csv\", index_label=False)\n",
    "del summary\n",
    "del teacher_student\n"
   ],
   "id": "fa8dc7a936249279",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "17a75d3f",
   "metadata": {},
   "source": "## 3.5 Format Data"
  },
  {
   "cell_type": "markdown",
   "id": "bb288780",
   "metadata": {},
   "source": [
    "In the previous phase we concluded that Country variable should be grouped into fewer values to avoid high dimensionality.\n",
    "\n",
    "To reduce the cardinality of the \"CNT\" (Country) variable while preserving meaningful educational information, countries were grouped into three categories based on their average PISA performance: Above Average, Average, and Below Average."
   ]
  },
  {
   "cell_type": "code",
   "id": "a379aadf",
   "metadata": {},
   "source": [
    "#load new table\n",
    "teacher_student = pd.read_csv('../../../databases/2018/integrated_teacher_student2018.csv')\n",
    "\n",
    "country_avg_scores = teacher_student.groupby(\"CNT\")[\"average_reading_score\"].mean()\n",
    "tolerance = 15\n",
    "global_avg = country_avg_scores.mean()\n",
    "\n",
    "def categorize_country(score):\n",
    "    if score >= global_avg + tolerance:\n",
    "        return \"Above Average\"\n",
    "    elif score <= global_avg - tolerance:\n",
    "        return \"Below Average\"\n",
    "    else:\n",
    "        return \"Average\"\n",
    "\n",
    "country_performance_group = country_avg_scores.apply(categorize_country)\n",
    "\n",
    "teacher_student[\"CNT_Group\"] = teacher_student[\"CNT\"].map(country_performance_group)\n",
    "teacher_student[\"CNT_Group\"] = teacher_student[\"CNT_Group\"].astype(\"category\")\n",
    "\n",
    "print(teacher_student[\"CNT_Group\"].value_counts())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def categorize_reading_teacher_by_teacher_performance(df):\n",
    "    # Sort the DataFrame by 'Avg Reading Result' in descending order\n",
    "    df = df.sort_values(by='average_reading_score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # Calculate the number of students\n",
    "    total = len(df)\n",
    "\n",
    "    # Define the percentage thresholds\n",
    "    very_good_threshold = int(total * 0.90)\n",
    "    good_threshold = int(total * 0.70)\n",
    "    sufficient_threshold = int(total * 0.50)\n",
    "\n",
    "    # Assign categories from highest to lowest\n",
    "    df.loc[0:sufficient_threshold-1, 'Reading Score Classification'] = 'Insufficient (0-49%)'\n",
    "    df.loc[sufficient_threshold:good_threshold-1, 'Reading Score Classification'] = 'Sufficient (50-69%)'\n",
    "    df.loc[good_threshold:very_good_threshold-1, 'Reading Score Classification'] = 'Good (70-89%)'\n",
    "    df.loc[very_good_threshold:, 'Reading Score Classification'] = 'Very Good (90-100%)'\n",
    "\n",
    "    return df\n",
    "\n",
    "teacher_student = categorize_reading_teacher_by_teacher_performance(teacher_student)\n",
    "\n",
    "print(\"Number of students by classification\")\n",
    "print(\"Very Good (90-100%): \", len(teacher_student[teacher_student['Reading Score Classification']==\"Very Good (90-100%)\"]))\n",
    "print(\"Good (70-89%): \", len(teacher_student[teacher_student['Reading Score Classification']==\"Good (70-89%)\"]))\n",
    "print(\"Sufficient (50-69%): \", len(teacher_student[teacher_student['Reading Score Classification']==\"Sufficient (50-69%)\"]))\n",
    "print(\"Insufficient (0-49%): \", len(teacher_student[teacher_student['Reading Score Classification']==\"Insufficient (0-49%)\"]))"
   ],
   "id": "649db5999d65126e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "03e10082",
   "metadata": {},
   "source": [
    "## Correlation chart\n",
    "\n",
    "Using a more cleaner dataset, we can check again the correlations between the dataset features and the target variable (average math result)."
   ]
  },
  {
   "cell_type": "code",
   "id": "65757db3",
   "metadata": {},
   "source": [
    "correl = (\n",
    "    teacher_student.corr(numeric_only=True)[\"average_reading_score\"]\n",
    "    .abs()\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "very_good_corr = teacher_student[teacher_student['Reading Score Classification']==\"Very Good (90-100%)\"].corr(numeric_only=True)[\n",
    "    \"average_reading_score\"\n",
    "].sort_values(ascending=False)\n",
    "best_performance.drop(\"average_reading_score\")\n",
    "\n",
    "best_performance = teacher_student[teacher_student['Reading Score Classification']==\"Very Good (90-100%)\"].corr(numeric_only=True)[\n",
    "    \"average_reading_score\"\n",
    "].sort_values(ascending=False)\n",
    "best_performance.drop(\"average_reading_score\")\n",
    "\n",
    "\n",
    "repeating_correlations = repeating_correlations.drop(\"Avg Math Result\")\n",
    "\n",
    "non_repeating_correlations = non_repeating_students.corr(numeric_only=True)[\n",
    "    \"Avg Math Result\"\n",
    "].sort_values(ascending=False)\n",
    "\n",
    "non_repeating_correlations = non_repeating_correlations.drop(\"Avg Math Result\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "60909b9f",
   "metadata": {},
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from dictionary import column_mapping\n",
    "\n",
    "repeating_correlations = repeating_correlations[repeating_correlations.abs() > 0.3]\n",
    "non_repeating_correlations = non_repeating_correlations[\n",
    "    non_repeating_correlations.abs() > 0.3\n",
    "]\n",
    "repeating_correlations.index = [\n",
    "    column_mapping.get(col, col) for col in repeating_correlations.index\n",
    "]\n",
    "non_repeating_correlations.index = [\n",
    "    column_mapping.get(col, col) for col in non_repeating_correlations.index\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "sns.barplot(x=repeating_correlations.values, y=repeating_correlations.index)\n",
    "plt.title(\"Math result correlation (Repeating Students)\")\n",
    "plt.ylabel(\"Numeric Variables\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "sns.barplot(x=non_repeating_correlations.values, y=non_repeating_correlations.index)\n",
    "plt.title(\"Math result correlation (Non-Repeating Students)\")\n",
    "plt.ylabel(\"Numeric Variables\")\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e1c8db64",
   "metadata": {},
   "source": [
    "<!-- Since the focus is on repeating students, we prioritized features that show a strong correlation with this group. -->\n",
    "\n",
    "Following the correlation analysis, we opted to incorporate variables identified as significant from both the repeating and non-repeating student groups. This mixed feature selection strategy aims to leverage complementary insights from both subsets, enhancing the robustness of the final dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e918426",
   "metadata": {},
   "source": [
    "## 3.1 Select Data (Cont.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3a002e",
   "metadata": {},
   "source": [
    "After completing the data analysis, we merged the selected numerical and categorical variables to create the final dataset, ready for the next phase."
   ]
  },
  {
   "cell_type": "code",
   "id": "b32e1a85",
   "metadata": {},
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "reversed_column_mapping = {v: k for k, v in column_mapping.items()}\n",
    "repeating_original_columns = [\n",
    "    reversed_column_mapping.get(col, col) for col in repeating_correlations.index\n",
    "]\n",
    "non_repeating_original_columns = [\n",
    "    reversed_column_mapping.get(col, col) for col in non_repeating_correlations.index\n",
    "]\n",
    "\n",
    "columns_to_include = list(\n",
    "    OrderedDict.fromkeys(\n",
    "        repeating_original_columns\n",
    "        + non_repeating_original_columns\n",
    "        + [\"Avg Math Result\", \"CNT_Group\"]\n",
    "    )\n",
    ")\n",
    "\n",
    "filtered_cols_dataset = filtered_student[columns_to_include].copy()\n",
    "filtered_cols_dataset = filtered_cols_dataset[sorted(filtered_cols_dataset.columns)]\n",
    "filtered_cols_dataset"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c13e8010",
   "metadata": {},
   "source": [
    "Finally, we analyzed the correlation matrix to check for highly correlated features that might be removed, but none were identified."
   ]
  },
  {
   "cell_type": "code",
   "id": "fcb4b41f",
   "metadata": {},
   "source": [
    "correlation_matrix = filtered_cols_dataset.corr(numeric_only=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "693faf65",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", square=True)\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "049ff82c",
   "metadata": {},
   "source": [
    "filtered_cols_dataset.to_csv(\n",
    "    \"../../../databases/q1_database.csv\",\n",
    "    index=False,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
